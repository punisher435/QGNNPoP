{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir='/usr/lib/cuda'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ac/popularity/code_submission/data/df_github.csv\", index_col=0)\n",
    "df = df.reset_index()\n",
    "df = df.drop(columns = [\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stars\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df\n",
    "\n",
    "createdAt = {}\n",
    "for idx,item in data_df[\"createdAt\"].items():\n",
    "    temp = item.split(\" \")\n",
    "    for i in range(len(temp)):\n",
    "        temp[i] = float(temp[i])\n",
    "    createdAt[str(idx)] = temp\n",
    "\n",
    "updatedAt = {}\n",
    "for idx,item in data_df[\"updatedAt\"].items():\n",
    "    temp = item.split(\" \")\n",
    "    for i in range(len(temp)):\n",
    "        temp[i] = float(temp[i])\n",
    "    updatedAt[str(idx)] = temp\n",
    "    \n",
    "pushedAt = {}\n",
    "for idx,item in data_df[\"pushedAt\"].items():\n",
    "    temp = item.split(\" \")\n",
    "    for i in range(len(temp)):\n",
    "        temp[i] = float(temp[i])\n",
    "    pushedAt[str(idx)] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean and largest number of words in description before pre processing\n",
    "sum1=0\n",
    "len1 = 0\n",
    "max1 = 0\n",
    "for idx,item in df[\"description\"].items():\n",
    "    temp = item.split(\" \")\n",
    "    len1+=1\n",
    "    sum1+=len(temp)\n",
    "    max1 = max(max1,len(temp))\n",
    "print(sum1/len1)\n",
    "print(max1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean and largest number of words in readme before pre processing\n",
    "sum1=0\n",
    "len1 = 0\n",
    "max1 = 0\n",
    "for idx,item in df[\"readme\"].items():\n",
    "    temp = item.split(\" \")\n",
    "    len1+=1\n",
    "    sum1+=len(temp)\n",
    "    max1 = max(max1,len(temp))\n",
    "print(sum1/len1)\n",
    "print(max1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean and largest number of words in readme before pre processing\n",
    "sum1=0\n",
    "len1 = 0\n",
    "max1 = 0\n",
    "for idx,item in df[\"processed_readme\"].items():\n",
    "    temp = item.split(\" \")\n",
    "    len1+=1\n",
    "    sum1+=len(temp)\n",
    "    max1 = max(max1,len(temp))\n",
    "print(sum1/len1)\n",
    "print(max1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean and largest number of words in readme before pre processing\n",
    "sum1=0\n",
    "len1 = 0\n",
    "max1 = 0\n",
    "for idx,item in df[\"summary_t5\"].items():\n",
    "    temp = item.split(\" \")\n",
    "    len1+=1\n",
    "    sum1+=len(temp)\n",
    "    max1 = max(max1,len(temp))\n",
    "print(sum1/len1)\n",
    "print(max1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_for_emb = []\n",
    "\n",
    "for idx,items in df[\"keywords_5_w\"].items():\n",
    "    str_t = \"\"\n",
    "    for item in items:\n",
    "        str_t += item[0]\n",
    "        str_t += \" \"\n",
    "    if len(str_t)>0:\n",
    "        str_t = str_t[:len(str_t)-1]\n",
    "    keywords_for_emb.append(str_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_stars = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in Num_dataset.columns:\n",
    "    if item == \"stars\":\n",
    "        continue\n",
    "    \n",
    "    if Num_dataset[item].dtype==float or Num_dataset[item].dtype == int:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(Num_dataset[item].values.reshape(-1,1))\n",
    "        Num_dataset[item] = scaler.transform(Num_dataset[item].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_stars.fit(Num_dataset[\"stars\"].values.reshape(-1,1))\n",
    "Num_dataset[\"stars\"] = scaler_stars.transform(Num_dataset[\"stars\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_dataset[\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_dataset[\"stars\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(Num_dataset[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(Num_dataset[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(Num_dataset.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f, spearmanr, pearsonr\n",
    "\n",
    "keep_arr = []\n",
    "\n",
    "pot_arr = []\n",
    "pot_score = []\n",
    "\n",
    "\n",
    "def s_test1(X,Y,column):\n",
    "    \n",
    "    try:\n",
    "        corr, p_v = spearmanr(X, Y)\n",
    "        if abs(corr)>=0.25:\n",
    "            keep_arr.append(column)\n",
    "        elif abs(corr)>=0.1 and abs(corr)<0.25:\n",
    "            pot_arr.append(column)\n",
    "            pot_score.append(corr)\n",
    "        print(column, \" \", corr, \" \", p_v)\n",
    "    except:\n",
    "        print(\"failed\")\n",
    "        \n",
    "def s_test(X,Y,column):\n",
    "    \n",
    "    try:\n",
    "        corr, p_v = spearmanr(X, Y)\n",
    "        print(column, \" \", corr, \" \", p_v)\n",
    "    except:\n",
    "        print(\"failed\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in Num_dataset.columns:\n",
    "    s_test1(Num_dataset[item],Num_dataset[\"stars\"],item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with a standard time\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "def calc_time(str):\n",
    "    arr = str.split(\" \")\n",
    "    d = datetime(int(arr[0]),int(arr[1]),int(arr[2]))\n",
    "    d1 = datetime(1983,1,1)\n",
    "    return (d-d1).days\n",
    "\n",
    "total_updated_time = []\n",
    "\n",
    "for idx,item in Num_dataset[\"updatedAt\"].items():\n",
    "    total_updated_time.append(calc_time(Num_dataset[\"updatedAt\"][idx]))\n",
    "\n",
    "total_updated_time_np = np.array(total_updated_time)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_u = MinMaxScaler()\n",
    "scaler_u.fit(total_updated_time_np.reshape(-1,1))\n",
    "total_updated_time_np = scaler_u.transform(total_updated_time_np.reshape(-1,1))\n",
    "\n",
    "\n",
    "s_test1(flatten(total_updated_time_np),np.array(Num_dataset[\"stars\"]),\"updated_At\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with a standard time\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "def calc_time(str):\n",
    "    arr = str.split(\" \")\n",
    "    d = datetime(int(arr[0]),int(arr[1]),int(arr[2]))\n",
    "    d1 = datetime(1983,1,1)\n",
    "    return (d-d1).days\n",
    "\n",
    "total_updated_time = []\n",
    "\n",
    "for idx,item in Num_dataset[\"updatedAt\"].items():\n",
    "    total_updated_time.append(calc_time(Num_dataset[\"updatedAt\"][idx]))\n",
    "\n",
    "total_updated_time_np = np.array(total_updated_time)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_u = MinMaxScaler()\n",
    "scaler_u.fit(total_updated_time_np.reshape(-1,1))\n",
    "total_updated_time_np = scaler_u.transform(total_updated_time_np.reshape(-1,1))\n",
    "\n",
    "\n",
    "s_test(total_updated_time_np,Num_dataset[\"stars\"],\"updated_At\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fei\n",
    "\n",
    "fei = []\n",
    "for idx,item in Num_dataset[\"forkCount\"].items():\n",
    "    xx = 0.63*float(Num_dataset[\"forkCount\"][idx] ) + (1-0.63)*float(Num_dataset[\"subscribersCount\"][idx]) \n",
    "    fei.append(xx)\n",
    "    \n",
    "fei = np.array(fei)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(fei.reshape(-1,1))\n",
    "fei = scaler.transform(fei.reshape(-1,1))\n",
    "\n",
    "s_test(fei,Num_dataset[\"stars\"],\"fei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rii\n",
    "\n",
    "rii = []\n",
    "for idx,item in Num_dataset[\"forkCount\"].items():\n",
    "    xx = float(Num_dataset[\"subscribersCount\"][idx] ) / (float(Num_dataset[\"forkCount\"][idx])+1) \n",
    "    rii.append(xx)\n",
    "    \n",
    "rii = np.array(rii)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(rii.reshape(-1,1))\n",
    "rii = scaler.transform(rii.reshape(-1,1))\n",
    "\n",
    "s_test(rii,Num_dataset[\"stars\"],\"rii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dei\n",
    "\n",
    "dei = []\n",
    "for idx,item in Num_dataset[\"forkCount\"].items():\n",
    "    xx = float(Num_dataset[\"forkCount\"][idx] ) + float(Num_dataset[\"prOpenCommits\"][idx] ) + float(Num_dataset[\"prOpenComments\"][idx]) + float(Num_dataset[\"prMerged\"][idx] ) + float(Num_dataset[\"prClosed\"][idx])  + float(Num_dataset[\"prMergedComments\"][idx] ) + float(Num_dataset[\"prMergedCommits\"][idx])  + float(Num_dataset[\"prOpen\"][idx]) + float(Num_dataset[\"prClosedComments\"][idx])  + float(Num_dataset[\"prClosedCommits\"][idx]) \n",
    "    f10.append(xx)\n",
    "\n",
    "dei = np.array(dei)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(dei.reshape(-1,1))\n",
    "dei = scaler.transform(dei.reshape(-1,1))\n",
    "\n",
    "s_test(dei,Num_dataset[\"stars\"],\"dei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_dataset[\"fei\"] = fei\n",
    "Num_dataset[\"rii\"] = rii\n",
    "Num_dataset[\"dei\"] = dei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_dataset[\"upd\"] = total_updated_time_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_thresholds = Num_dataset[\"stars\"].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "\n",
    "for star in Num_dataset[\"stars\"]:\n",
    "    if star <= class_thresholds[0.5]:\n",
    "        count0 += 1\n",
    "    else:\n",
    "        count1 += 1\n",
    "        \n",
    "print(count0,\" \",count1)\n",
    "\n",
    "def makeClasses(star):\n",
    "    if star <= class_thresholds[0.5]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "starsArray = np.array([makeClasses(star) for star in Num_dataset[\"stars\"]])\n",
    "\n",
    "hot_encode_result = []\n",
    "\n",
    "for i in range(len(starsArray)):\n",
    "    if starsArray[i] == 0:\n",
    "        hot_encode_result.append([Num_dataset[\"stars\"][i],1.0,0.0])\n",
    "    elif starsArray[i] == 1:\n",
    "        hot_encode_result.append([Num_dataset[\"stars\"][i],0.0,1.0])\n",
    "        \n",
    "Num_dataset[\"class\"] = starsArray\n",
    "        \n",
    "hot_encode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(Num_dataset, test_size=0.2, random_state=42)\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.drop(columns = [\"index\"])\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns = [\"index\"])\n",
    "\n",
    "df_train.to_csv(\"df_github_train.csv\")\n",
    "df_test.to_csv(\"df_github_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
