{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir='/usr/lib/cuda'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tensorflow():\n",
    "    # Filter tensorflow version warnings\n",
    "    import os\n",
    "    # https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "    import warnings\n",
    "    # https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=Warning)\n",
    "    import tensorflow as tf\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "    tf.autograph.set_verbosity(0)\n",
    "    import logging\n",
    "    tf.get_logger().setLevel(logging.ERROR)\n",
    "    return tf\n",
    "\n",
    "tf = import_tensorflow() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = \"/home/ac/popularity/code_submission/data/df_github_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = \"/home/ac/popularity/code_submission/data/df_github_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_df, index_col=0)\n",
    "tdf = pd.read_csv(test_df, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph\n",
    "import ast\n",
    "import string\n",
    "\n",
    "final_keywords = []\n",
    "\n",
    "keywords_w = []\n",
    "\n",
    "for idx,item in df[\"keywords\"].items():\n",
    "    my_list = ast.literal_eval(item)\n",
    "    data = my_list\n",
    "    new_data = []\n",
    "    for it in data:\n",
    "        new_data.append((it[0].lower(),it[1]))\n",
    "    keywords_w.append(new_data)\n",
    "    \n",
    "df[\"keywords_5_w\"] = keywords_w\n",
    "\n",
    "keywords_temp = dict(df[\"keywords_5_w\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph test\n",
    "import ast\n",
    "import string\n",
    "\n",
    "final_keywords = []\n",
    "\n",
    "keywords_w = []\n",
    "\n",
    "for idx,item in tdf[\"keywords\"].items():\n",
    "    my_list = ast.literal_eval(item)\n",
    "    data = my_list\n",
    "    new_data = []\n",
    "    for it in data:\n",
    "        new_data.append((it[0].lower(),it[1]))\n",
    "    keywords_w.append(new_data)\n",
    "    \n",
    "tdf[\"keywords_5_w\"] = keywords_w\n",
    "\n",
    "tkeywords_temp = dict(tdf[\"keywords_5_w\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[\"keywords_5_w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read h5 files\n",
    "\n",
    "def getH5Data(fileName):\n",
    "    myDict = {}\n",
    "    f = h5py.File(fileName, \"r\")\n",
    "    keys = list(f.keys())\n",
    "    for k in keys:\n",
    "        myDict[k] = np.array(f[k])\n",
    "    f.close()\n",
    "    return myDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Readme_embeddings_pd = getH5Data(\"/home/ac/popularity/code_submission/embeddings/mbert_readme_features.h5\")\n",
    "\n",
    "Readme_embeddings_pd['0'].shape\n",
    "\n",
    "#for test\n",
    "tReadme_embeddings_pd = getH5Data(\"/home/ac/popularity/code_submission/embeddings/tmbert_readme_features.h5\")\n",
    "\n",
    "tReadme_embeddings_pd['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Description_embeddings_pd = getH5Data(\"/home/ac/popularity/code_submission/embeddings/mbert_description_features.h5\")\n",
    "\n",
    "Description_embeddings_pd['0'].shape\n",
    "\n",
    "#for test\n",
    "tDescription_embeddings_pd = getH5Data(\"/home/ac/popularity/code_submission/embeddings/tmbert_description_features.h5\")\n",
    "\n",
    "tDescription_embeddings_pd['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords_embeddings_pd = getH5Data(\"/home/ac/popularity/code_submission/embeddings/mbert_keyword_features.h5\")\n",
    "\n",
    "Keywords_embeddings_pd['0'].shape\n",
    "\n",
    "#for test\n",
    "tKeywords_embeddings_pd = getH5Data(\"/home/ac/popularity/code_submission/embeddings/tmbert_keyword_features.h5\")\n",
    "\n",
    "tKeywords_embeddings_pd['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_shape = Readme_embeddings_pd[list(Readme_embeddings_pd.keys())[0]].shape\n",
    "de_shape = Description_embeddings_pd[list(Description_embeddings_pd.keys())[0]].shape\n",
    "key_shape = Keywords_embeddings_pd[list(Description_embeddings_pd.keys())[0]].shape\n",
    "print(re_shape)\n",
    "print(de_shape)\n",
    "print(key_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_keys = Readme_embeddings_pd.keys()\n",
    "de_keys = Description_embeddings_pd.keys()\n",
    "\n",
    "tre_keys = tReadme_embeddings_pd.keys()\n",
    "tde_keys = tDescription_embeddings_pd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_keys = list(re_keys)\n",
    "re_t_keys = []\n",
    "for i in re_keys:\n",
    "    re_t_keys.append(int(i))\n",
    "    \n",
    "tre_keys = list(tre_keys)\n",
    "tre_t_keys = []\n",
    "for i in tre_keys:\n",
    "    tre_t_keys.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainkeys = re_t_keys\n",
    "testkeys = tre_t_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer,\n",
    "    InputSpec,\n",
    "    Attention,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Softmax,\n",
    "    Activation,\n",
    "    Reshape,\n",
    "    RepeatVector,\n",
    "    Permute,\n",
    "    Dot,\n",
    "    MultiHeadAttention,\n",
    "    LayerNormalization,\n",
    ")\n",
    "import tensorflow.keras.utils as conv_utils\n",
    "from tensorflow.keras import activations, initializers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers as initializers, regularizers, constraints\n",
    "\n",
    "REMOVE_FACTOR = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class HierarchicalAttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(HierarchicalAttentionLayer())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(HierarchicalAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape = (input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape = (input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(HierarchicalAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(HierarchicalAttentionLayer, self).get_config()\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number e to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_between_repo (krepo1,krepo2):\n",
    "    sum_s = 0\n",
    "    i=0\n",
    "    for item1 in krepo1:\n",
    "        for item2 in krepo2:\n",
    "            if item1[0] == item2[0]:\n",
    "                i+=1\n",
    "                sum_s+=item1[1]\n",
    "                sum_s+=item2[1]\n",
    "    if i==0:\n",
    "        i=1\n",
    "    return float(sum_s)/float(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_with_repo_nodes (topics):\n",
    "\n",
    "    nodes = len(topics)\n",
    "    weights = []\n",
    "    edges = []\n",
    "    \n",
    "    for i in range(len(topics)):\n",
    "        for j in range(len(topics)):\n",
    "            simi = get_similarity_between_repo(topics[i],topics[j])\n",
    "            if simi>0:\n",
    "                edges.append((i,j))\n",
    "                weights.append(simi)\n",
    "    return edges,weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(Num_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starsArray = df[\"class\"]\n",
    "\n",
    "hot_encode_result = []\n",
    "\n",
    "for i in range(len(starsArray)):\n",
    "    if starsArray[i] == 0:\n",
    "        hot_encode_result.append([Num_dataset[\"stars\"][i],1.0,0.0])\n",
    "    elif starsArray[i] == 1:\n",
    "        hot_encode_result.append([Num_dataset[\"stars\"][i],0.0,1.0])\n",
    "        \n",
    "        \n",
    "hot_encode_result\n",
    "\n",
    "\n",
    "thot_encode_result = []\n",
    "\n",
    "for i in range(len(tstarsArray)):\n",
    "    if tstarsArray[i] == 0:\n",
    "        thot_encode_result.append([tdf[\"stars\"][i],1.0,0.0])\n",
    "    elif tstarsArray[i] == 1:\n",
    "        thot_encode_result.append([tdf[\"stars\"][i],0.0,1.0])\n",
    "        \n",
    "        \n",
    "thot_encode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_dataset.drop(columns=[\"description\",\"readme\",\"keywords_5_w\",\"class\",\"updatedAt\",\"createdAt\",\"pushedAt\",\"processed_readme\",\"summary_t5\",\"login\",\"reponame\",\"keywords\"])\n",
    "tdf.drop(columns=[\"description\",\"readme\",\"keywords_5_w\",\"class\",\"updatedAt\",\"createdAt\",\"pushedAt\",\"processed_readme\",\"summary_t5\",\"login\",\"reponame\",\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict(zip(range(len(Num_dataset[\"stars\"])),hot_encode_result))\n",
    "Num_temp = Num_dataset.drop(columns=[\"stars\"])\n",
    "Num_dict = Num_temp.T.to_dict('list')\n",
    "\n",
    "tresult_dict = dict(zip(range(len(tdf[\"stars\"])),thot_encode_result))\n",
    "tNum_temp = tdf.drop(columns=[\"stars\"])\n",
    "tNum_dict = tNum_temp.T.to_dict('list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QCNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "def state_prepare(input, qbits):\n",
    "    qml.templates.AngleEmbedding(input, wires=range(qbits))\n",
    "\n",
    "# Define quantum convolution and pooling functions\n",
    "\n",
    "def conv(b1, b2, params):\n",
    "    qml.RZ(-tf.constant(np.pi/2), wires=b2)\n",
    "    qml.CNOT([b2, b1])\n",
    "    qml.RZ(params[0], wires=b1)\n",
    "    qml.RY(params[1], wires=b2)\n",
    "    qml.CNOT([b1, b2])\n",
    "    qml.RY(params[2], wires=b2)\n",
    "    qml.CNOT([b2, b1])\n",
    "    qml.RZ(tf.constant(np.pi/2), wires=b1)\n",
    "\n",
    "def pool(b1, b2, params):\n",
    "    qml.RZ(-tf.constant(np.pi/2), wires=b2)\n",
    "    qml.CNOT([b2, b1])\n",
    "    qml.RZ(params[0], wires=b1)\n",
    "    qml.RY(params[1], wires=b2)\n",
    "    qml.CNOT([b1, b2])\n",
    "    qml.RY(params[2], wires=b2)\n",
    "\n",
    "dev4 = qml.device('default.qubit.tf', wires=5)\n",
    "\n",
    "@qml.qnode(dev4, interface='tf')\n",
    "def qcnn4(weights, inputs):\n",
    "    state_prepare(inputs, 4)\n",
    "    conv(0, 1, weights[0:3])\n",
    "    conv(2, 3, weights[3:6])\n",
    "    pool(0, 1, weights[6:9])\n",
    "    pool(2, 3, weights[9:12])\n",
    "    conv(1, 3, weights[12:15])\n",
    "    pool(1, 3, weights[15:18])\n",
    "    return qml.expval(qml.PauliX(3))\n",
    "\n",
    "dev8 = qml.device('default.qubit.tf', wires=9)\n",
    "\n",
    "@qml.qnode(dev8, interface='tf')\n",
    "def qcnn8(weights, inputs):\n",
    "    state_prepare(inputs, 8)\n",
    "    conv(0,1,weights[0:3])\n",
    "    pool(0,1,weights[3:6])\n",
    "    conv(2,3,weights[6:9])\n",
    "    pool(2,3,weights[9:12])\n",
    "    conv(4,5,weights[12:15])\n",
    "    pool(4,5,weights[15:18])\n",
    "    conv(6,7,weights[18:21])\n",
    "    pool(6,7,weights[21:24])\n",
    "    conv(1,3,weights[24:27])\n",
    "    pool(1,3,weights[27:30])\n",
    "    conv(5,7,weights[30:33])\n",
    "    pool(5,7,weights[33:36])\n",
    "    conv(3,7,weights[36:39])\n",
    "    pool(3,7,weights[39:42])\n",
    "    return [qml.expval(qml.PauliX(7))]\n",
    "\n",
    "dev8m = qml.device('default.qubit.tf', wires=9)\n",
    "@qml.qnode(dev8m, interface='tf')\n",
    "def qcnnmulti8(weights, inputs):\n",
    "    state_prepare(inputs, 8)\n",
    "\n",
    "    conv(0, 1, weights[0:3])\n",
    "    pool(0, 1, weights[3:6])\n",
    "    conv(2, 3, weights[6:9])\n",
    "    pool(2, 3, weights[9:12])\n",
    "    conv(1, 3, weights[12:15])\n",
    "    pool(1, 3, weights[15:18]) \n",
    "\n",
    "    conv(4,5,weights[18:21])\n",
    "    pool(4,5,weights[21:24])\n",
    "    conv(6,7,weights[24:27])\n",
    "    pool(6,7,weights[27:30])\n",
    "    conv(5,7,weights[30:33])\n",
    "    pool(5,7,weights[33:36]) \n",
    "\n",
    "    conv(3,7,weights[36:39])\n",
    "    pool(3,7,weights[39:42]) \n",
    "    \n",
    "    return [qml.expval(qml.PauliX(7)), qml.probs(wires=[3])] \n",
    "   \n",
    "\n",
    "class QCNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, qbits,regression):\n",
    "        super().__init__()\n",
    "        self.qbits = qbits\n",
    "        self.regression = regression\n",
    "\n",
    "        if qbits == 4:\n",
    "            self.qcnn_layer = qml.qnn.KerasLayer(qcnn4, weight_shapes={'weights': 18}, output_dim=1,dtype=tf.float64)\n",
    "        elif qbits == 8 and regression:\n",
    "            self.qcnn_layer = qml.qnn.KerasLayer(qcnn8, weight_shapes={'weights': 42}, output_dim=1,dtype=tf.float64)\n",
    "        elif qbits == 8:\n",
    "            self.qcnn_layer = qml.qnn.KerasLayer(qcnnmulti8, weight_shapes={'weights': 42}, output_dim=3,dtype=tf.float64)\n",
    "\n",
    "    def call(self, x):\n",
    "        max_val = tf.reduce_max(x)\n",
    "        min_val = tf.reduce_min(x)\n",
    "        \n",
    "        x = (x - min_val) / (max_val - min_val)\n",
    "        x = 2 * x * tf.constant(np.pi)\n",
    "\n",
    "        # Check if the input shape matches the qubits\n",
    "        if x.shape[1] > self.qbits:\n",
    "            x = x[:, :self.qbits]\n",
    "        elif x.shape[1] < self.qbits:\n",
    "            pad_width = self.qbits - x.shape[1]\n",
    "            x = tf.pad(x, [[0, 0], [0, pad_width]])\n",
    "\n",
    "        # Apply the quantum layer\n",
    "        quantum_output = self.qcnn_layer(x)\n",
    "        \n",
    "        # Ensure the output shape matches (batch_size, 2)\n",
    "        return quantum_output\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = 3 # 5\n",
    "        if self.regression:\n",
    "            output_shape = 1\n",
    "        return tf.TensorShape([None,output_shape])\n",
    "\n",
    "    def count_params(self):\n",
    "        if not self.qcnn_layer.built:\n",
    "            self.qcnn_layer.build((None, self.qbits))  # Input shape with the correct number of qubits\n",
    "        return self.qcnn_layer.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Softmax, Activation, Reshape, RepeatVector, \\\n",
    "            Multiply, Permute, Input, Concatenate, Embedding, Attention, Masking, Flatten, Lambda, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import tensorflow.keras.utils as conv_utils\n",
    "from tensorflow.keras import activations, initializers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import tensorflow.keras.utils as conv_utils\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras import initializers as initializers, regularizers, constraints\n",
    "\n",
    "REMOVE_FACTOR = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import  spearmanr\n",
    "\n",
    "def get_spearman_rank (x,y,axis =0):\n",
    "    corr, p_v = spearmanr(x, y,axis)\n",
    "    return corr,p_v\n",
    "\n",
    "def calc_mse(y_true,y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true[:,0],y_pred[:,0])\n",
    "\n",
    "def calc_mae(y_true,y_pred):\n",
    "    return tf.keras.losses.mean_absolute_error(y_true[:,0],y_pred[:,0])\n",
    "\n",
    "def calc_catloss(y_true,y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true[:,1:],y_pred[:,1:])\n",
    "\n",
    "def calc_acc(y_true,y_pred):\n",
    "    return tf.keras.metrics.categorical_accuracy(y_true[:,1:],y_pred[:,1:])\n",
    "\n",
    "def customLoss(y_true, y_pred):\n",
    "    catloss = calc_catloss(y_true,y_pred)    \n",
    "    return catloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_readme = True\n",
    "include_description = True\n",
    "include_graph_desc = True\n",
    "include_numerical = True\n",
    "include_key = True\n",
    "\n",
    "num_re = re_shape[0]\n",
    "re_embedding_size = re_shape[1]\n",
    "print(num_re,re_embedding_size)\n",
    "\n",
    "num_de=de_shape[0]\n",
    "de_embedding_size = de_shape[1]\n",
    "print(num_de,de_embedding_size)\n",
    "\n",
    "num_key = key_shape[0]\n",
    "keywords_embedding_size = key_shape[1]\n",
    "print(num_key,keywords_embedding_size)\n",
    "\n",
    "num_numdataset = Num_temp.shape[1]\n",
    "\n",
    "batch_size = 32\n",
    "test_batch_size = batch_size\n",
    "epochs = 20\n",
    "drop_rate = 0.2\n",
    "attention_size = 300\n",
    "embedding_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRepoFeature(inputs_description,inputs_readme,inputs_numerical, inputs_keywords,layers):\n",
    "\n",
    "    if inputs_description is not None:\n",
    "        deFeature = layers[1](inputs_description)\n",
    "    else:\n",
    "        deFeature = None\n",
    "    \n",
    "    if inputs_readme is not None:\n",
    "        reFeature = layers[0](inputs_readme)\n",
    "    else:\n",
    "        reFeature = None\n",
    "        \n",
    "    if inputs_numerical is not None:\n",
    "        numFeature = (inputs_numerical)\n",
    "    else:\n",
    "        numFeature = None\n",
    "        \n",
    "    if inputs_keywords is not None:\n",
    "        keyFeature = layers[2](inputs_keywords)\n",
    "    else:\n",
    "        keyFeature = None\n",
    "\n",
    "    return reFeature, keyFeature, deFeature,  numFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import GraphSageConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainModelDef():\n",
    "    finalInputs = []\n",
    "    \n",
    "    if include_readme:\n",
    "        inputs_readme = Input(shape=(num_re,re_embedding_size), name='readme_input')\n",
    "        finalInputs.append(inputs_readme)\n",
    "    else:\n",
    "        inputs_readme = None\n",
    "\n",
    "    if include_description:\n",
    "        inputs_description = Input(shape=(num_de,de_embedding_size), name='description_input')\n",
    "        finalInputs.append(inputs_description)\n",
    "    else:\n",
    "        inputs_description = None\n",
    "        \n",
    "    if include_key:\n",
    "        inputs_keywords= Input(shape=(num_key,keywords_embedding_size), name='keywords_input')\n",
    "        finalInputs.append(inputs_keywords)\n",
    "    else:\n",
    "        inputs_keywords = None\n",
    "    \n",
    "    if include_key:\n",
    "        inputs_sparse= Input(shape=(batch_size,), name='query_sparse_input_1', sparse=True, dtype=tf.float64)\n",
    "        finalInputs.append(inputs_sparse)\n",
    "    else:\n",
    "        inputs_sparse = None\n",
    "    \n",
    "    if include_numerical:\n",
    "        inputs_numerical = Input(shape=(num_numdataset,), name='numerical_input')\n",
    "        finalInputs.append(inputs_numerical)\n",
    "    else:\n",
    "        inputs_numerical = None\n",
    "\n",
    "        \n",
    "    descriptionAtt = HierarchicalAttentionLayer(name='desc_Att')\n",
    "    readmeAtt = HierarchicalAttentionLayer(name='readme_Att')\n",
    "    keyAtt = HierarchicalAttentionLayer(name='key_Att')\n",
    "    \n",
    "    layers = [readmeAtt, descriptionAtt,keyAtt]\n",
    "\n",
    "    reFeature,  keyFeature,deFeature, numFeature  = getRepoFeature(inputs_description,inputs_readme,inputs_numerical, inputs_keywords, layers)\n",
    "    \n",
    "    txt_ft_t = Concatenate()([reFeature,deFeature,keyFeature])\n",
    "    \n",
    "    g_feat2 = GraphSageConv(channels=3*embedding_size)([txt_ft_t, inputs_sparse])\n",
    "    \n",
    "    txt_ft = Concatenate()([g_feat2])\n",
    "    \n",
    "    h = Concatenate()([numFeature,txt_ft])\n",
    "    \n",
    "    d1 = Dense(2320, activation=\"swish\")(h)\n",
    "    d11 = Dropout(drop_rate)(d1)\n",
    "    d1_1 = Dense(1160, activation=\"swish\")(d11)\n",
    "    d11_1 = Dropout(drop_rate)(d1_1)\n",
    "    d2 = Dense(580, activation=\"swish\")(d11_1)\n",
    "    d21 = Dropout(drop_rate)(d2)\n",
    "    d3 = Dense(290, activation=\"swish\")(d21)\n",
    "    d31 = Dropout(drop_rate)(d3)\n",
    "    d4 = Dense(145, activation=\"swish\")(d31)\n",
    "    d41 = Dropout(drop_rate)(d4)\n",
    "    d5 = Dense(72, activation=\"swish\")(d41)\n",
    "    d51 = Dropout(drop_rate)(d5)\n",
    "    d6= Dense(36, activation=\"swish\")(d51)\n",
    "    d61 = Dropout(drop_rate)(d6)\n",
    "    d7 = Dense(18, activation=\"swish\")(d61)\n",
    "    d71 = Dropout(drop_rate)(d7)\n",
    "    d8 = Dense(8, activation=\"swish\")(d71)\n",
    "\n",
    "    qnn = QCNN(8,False)\n",
    "    q1 = qnn(d8)\n",
    "\n",
    "    model = Model(inputs=finalInputs, outputs=q1)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.AdamW(0.001), loss=customLoss,\n",
    "    metrics = [calc_catloss, calc_acc,calc_mse,calc_mae])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modelTrainBatchMaker(reFeat,deFeat,keyFeat, Num_dict, keys,result_dict, keywords_for_graph, isRandom=True):\n",
    "    keys_copy = keys.copy()\n",
    "    deFeat_copy = deFeat.copy()\n",
    "    reFeat_copy = reFeat.copy()\n",
    "    keyFeat_copy = keyFeat.copy()\n",
    "    result_dict_copy = result_dict.copy()\n",
    "    Num_dict_copy = Num_dict.copy()\n",
    "    keywords_for_graph_copy = keywords_for_graph.copy()\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        if i + batch_size <= len(keys_copy):\n",
    "           \n",
    "            reqKeys = keys_copy[i:i+batch_size]\n",
    "            \n",
    "\n",
    "            inp = []\n",
    "            \n",
    "            if include_readme:\n",
    "                v = np.array([reFeat_copy[str(k)] for k in reqKeys]) \n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1], 768))\n",
    "                inp.append(v)\n",
    "                \n",
    "            if include_description:\n",
    "                v = np.array([deFeat_copy[str(k)] for k in reqKeys]) \n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1], 768))\n",
    "                inp.append(v)\n",
    "                \n",
    "            if include_key:\n",
    "                v = np.array([keyFeat_copy[str(k)] for k in reqKeys]) \n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1], 768))\n",
    "                inp.append(v)\n",
    "            \n",
    "            if include_key:\n",
    "                topics = [keywords_for_graph_copy[int(k)] for k in reqKeys]\n",
    "                edges, weights = make_graph_with_repo_nodes(topics)\n",
    "                sparseInp = tf.SparseTensor(\n",
    "                                indices = edges,\n",
    "                                values = weights,\n",
    "                                dense_shape = (batch_size,batch_size)\n",
    "                            )\n",
    "\n",
    "                inp.append(sparseInp)\n",
    "                \n",
    "                \n",
    "            if include_numerical:\n",
    "                v = np.array([Num_dict_copy[int(k)] for k in reqKeys])\n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1]))\n",
    "                inp.append(v)\n",
    "    \n",
    "                \n",
    "            pop = np.array([result_dict_copy[int(k)] for k in reqKeys])\n",
    "            \n",
    "            i+=batch_size\n",
    "            yield inp,np.array(pop)\n",
    "        else:\n",
    "            i = 0\n",
    "            continue\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modelTestBatchMaker(reFeat,deFeat,keyFeat, Num_dict, keys, result_dict, keywords_for_graph, isRandom=True):\n",
    "    keys_copy = keys.copy()\n",
    "    deFeat_copy = deFeat.copy()\n",
    "    reFeat_copy = reFeat.copy()\n",
    "    keyFeat_copy = keyFeat.copy()\n",
    "    result_dict_copy = result_dict.copy()\n",
    "    Num_dict_copy = Num_dict.copy()\n",
    "    keywords_for_graph_copy = keywords_for_graph.copy()\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        if i + batch_size <= len(keys_copy):\n",
    "            reqKeys = keys_copy[i:i+batch_size]\n",
    "\n",
    "            inp = []\n",
    "            \n",
    "            if include_readme:\n",
    "                v = np.array([reFeat_copy[str(k)] for k in reqKeys]) \n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1], 768))\n",
    "                inp.append(v)\n",
    "                \n",
    "            if include_description:\n",
    "                v = np.array([deFeat_copy[str(k)] for k in reqKeys]) \n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1], 768))\n",
    "                inp.append(v)\n",
    "                \n",
    "            if include_key:\n",
    "                v = np.array([keyFeat_copy[str(k)] for k in reqKeys]) \n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1], 768))\n",
    "                inp.append(v)\n",
    "                \n",
    "            \n",
    "            if include_key:\n",
    "                topics = [keywords_for_graph_copy[int(k)] for k in reqKeys]\n",
    "                edges, weights = make_graph_with_repo_nodes(topics)\n",
    "                sparseInp = tf.SparseTensor(\n",
    "                                indices = edges,\n",
    "                                values = weights,\n",
    "                                dense_shape = (batch_size,batch_size)\n",
    "                            )\n",
    "\n",
    "                inp.append(sparseInp)\n",
    "                \n",
    "                \n",
    "            if include_numerical:\n",
    "                v = np.array([Num_dict_copy[int(k)] for k in reqKeys])\n",
    "                v = np.reshape(v,(v.shape[0], v.shape[1]))\n",
    "                inp.append(v)\n",
    "                \n",
    "                \n",
    "            pop = np.array([result_dict_copy[int(k)] for k in reqKeys])\n",
    "            i+=batch_size\n",
    "            \n",
    "            yield inp,np.array(pop)\n",
    "        else:\n",
    "            i = 0\n",
    "            continue\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFile='q_classify.h5'\n",
    "resFile='q_classify.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mainModelDef()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "acc = 0\n",
    "for e in range(50):\n",
    "    history = model.fit(x=modelTrainBatchMaker(Readme_embeddings_pd,Description_embeddings_pd,Keywords_embeddings_pd, Num_dict, trainkeys,result_dict, keywords_temp),\n",
    "                        steps_per_epoch=len(trainkeys) / batch_size,\n",
    "                        epochs=1,\n",
    "                        verbose=1)\n",
    "    \n",
    "    pred=model.evaluate(x = modelTrainBatchMaker( tReadme_embeddings_pd,tDescription_embeddings_pd,tKeywords_embeddings_pd, tNum_dict,  testkeys,tresult_dict, tkeywords_temp),\n",
    "                      steps=len(testkeys) / batch_size, \n",
    "                      verbose=1)\n",
    "\n",
    "\n",
    "    resString = \"Epoch: \"+ str(e+1) + \"\\t\" + \"Cat loss: \" + str(pred[1]) + \"\\t\" + \"Acc: \" + str(pred[2])  +\"\\n\"\n",
    "    \n",
    "    print(resString, end=\"\")\n",
    "    \n",
    "    if pred[2]>acc:\n",
    "        acc=pred[2]\n",
    "        model.save_weights(modelFile)\n",
    "        g=open(resFile,'a')\n",
    "        g.write(resString)\n",
    "        g.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel = mainModelDef()\n",
    "bestmodel.load_weights(modelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = {}\n",
    "test_true = []\n",
    "\n",
    "pred=bestmodel.predict(x = modelTestBatchMaker(tReadme_embeddings_pd,tDescription_embeddings_pd,tKeywords_embeddings_pd, tNum_dict,  testkeys,tresult_dict, tkeywords_temp),\n",
    "                      steps=len(testkeys) / batch_size, \n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_res = pred[:,1:].argmax(1)\n",
    "true_res = np.array(test_true[:,1:]).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(true_res, predicted_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
