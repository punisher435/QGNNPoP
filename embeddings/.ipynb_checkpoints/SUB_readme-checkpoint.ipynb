{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir='/usr/lib/cuda'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tensorflow():\n",
    "    # Filter tensorflow version warnings\n",
    "    import os\n",
    "    # https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "    import warnings\n",
    "    # https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=Warning)\n",
    "    import tensorflow as tf\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "    tf.autograph.set_verbosity(0)\n",
    "    import logging\n",
    "    tf.get_logger().setLevel(logging.ERROR)\n",
    "    return tf\n",
    "\n",
    "tf = import_tensorflow() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = \"/home/ac/popularity/code_submission/data/df_github_train.csv\"\n",
    "#df_github_test.csv for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_df, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resFile_pre1 = '/home/ac/popularity/code_submission/fine_tuning/BERT_DESCRIPTION_SAVE_PRETRAINED'\n",
    "resFile_pre2 = '/home/ac/popularity/code_submission/fine_tuning/BERT_README_SAVE_PRETRAINED'\n",
    "resFile_pre3 = '/home/ac/popularity/code_submission/fine_tuning/BERT_KEYWORDS_SAVE_PRETRAINED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "from transformers import TFBertModel, BertConfig\n",
    "from transformers import TFRobertaModel\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def get_readme_encoding(data,name1,name2):\n",
    "    seq_length = 50\n",
    "    newDataFile = name1\n",
    "    textFeatPath = name2\n",
    "    tokenizer = AutoTokenizer.from_pretrained(resFile_pre2)\n",
    "    \n",
    "    def text_to_string(textStrings):\n",
    "        textArray = []\n",
    "        for textString in textStrings:\n",
    "            d = str(textString)\n",
    "            d = d.lower()\n",
    "\n",
    "            d = d.replace(\". \",\". [SEP] \")\n",
    "\n",
    "            # Add the special tokens.\n",
    "            marked_text = \"[CLS] \" + d + \" [SEP]\"\n",
    "\n",
    "            # Split the sentence into tokens.\n",
    "            tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "            # Map the token strings to their vocabulary indices.\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "            x = \"\"\n",
    "            for token in indexed_tokens:\n",
    "                x += str(token) + ' '\n",
    "\n",
    "            x = x[:-1]\n",
    "\n",
    "            textArray.append(x)\n",
    "\n",
    "        return textArray\n",
    "    \n",
    "    def textEncoderDef():\n",
    "        inputs_text = Input(shape=(seq_length,), dtype=tf.int32, name='query_text_input')\n",
    "        encoder = TFBertModel.from_pretrained(resFile_pre2)\n",
    "        encoder.bert.trainable = False\n",
    "        feat = Masking(mask_value=0)(encoder(inputs_text).last_hidden_state)\n",
    "        model = Model(inputs=inputs_text, outputs=feat)\n",
    "        return model\n",
    "\n",
    "    textEncoder = textEncoderDef()\n",
    "\n",
    "    tokenizedText = text_to_string(data)\n",
    "    ll = len(data)\n",
    "    new_col = []\n",
    "    for i in range(ll):\n",
    "        new_col.append(i)\n",
    "    tids = new_col\n",
    "\n",
    "    newData = []\n",
    "\n",
    "    for i in range(len(tids)):\n",
    "        newData.append(str(tids[i])+\"\\t\"+tokenizedText[i]+\"\\n\")\n",
    "\n",
    "    f = open(newDataFile, \"w\")\n",
    "    for d in newData:\n",
    "        f.write(d)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    textData = {}\n",
    "\n",
    "    f = open(newDataFile)\n",
    "    for l in f.readlines():\n",
    "        a = l.split(\"\\t\")\n",
    "        textData[a[0]] = [int(t) for t in a[1].split(\" \")]\n",
    "    f.close()\n",
    "    a = np.array([len(textData[k]) for k in textData])\n",
    "    np.percentile(a, [0,25,50,75,100])\n",
    "\n",
    "\n",
    "\n",
    "    def padTruncate(arr, reqlen):\n",
    "        a = arr.copy()[:reqlen]\n",
    "        while len(a) < reqlen:\n",
    "            a.append(0)\n",
    "        return a\n",
    "\n",
    "    def getEstimatedTime(done, doneTime, pending):\n",
    "        if done != 0:\n",
    "            return (doneTime*pending)/done\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def progressBar(perc, startTime):\n",
    "        clear_output(wait=True)\n",
    "        print(\"[\",end=\"\")\n",
    "        for j in range(30):\n",
    "            if perc > (j+2)*int(100/30):\n",
    "                print(\"=\",end=\"\")\n",
    "            elif perc > (j+1)*int(100/30):\n",
    "                print(\">\",end=\"\")\n",
    "            else:\n",
    "                print(\".\",end=\"\")\n",
    "        print(\"] \"+str(round(perc,2))+\"%\\t\",end=\"\")\n",
    "        doneTime = time.time() - startTime\n",
    "        print(\"Est Time: \"+str(round(getEstimatedTime(perc, doneTime, 100-perc),2))+\"s\")\n",
    "\n",
    "    def getDataInBatches(keys):\n",
    "        tData = []\n",
    "        for k in keys:\n",
    "            tData.append(np.array(padTruncate(textData[k], seq_length)))\n",
    "        tData = np.array(tData)\n",
    "        return tData\n",
    "\n",
    "    keysDone = {}\n",
    "\n",
    "    try:\n",
    "        f = h5py.File(textFeatPath, \"r\")\n",
    "        for k in list(f.keys()):\n",
    "            keysDone[k] = 1\n",
    "        f.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    keys = list(textData.keys())\n",
    "    keys2 = []\n",
    "\n",
    "    for k in keys:\n",
    "        try:\n",
    "            keysDone[k]\n",
    "        except:\n",
    "            keys2.append(k)\n",
    "\n",
    "    keys = keys2\n",
    "\n",
    "    c = 0\n",
    "    start = time.time()\n",
    "    total = len(keys)\n",
    "    batchSize = 64\n",
    "\n",
    "    numBatches = math.ceil(total / batchSize)\n",
    "\n",
    "    f = h5py.File(textFeatPath, \"a\")\n",
    "\n",
    "\n",
    "    for i in range(numBatches):\n",
    "        currKeys = keys[i*batchSize : (i+1)*batchSize]\n",
    "        features = textEncoder(getDataInBatches(currKeys))\n",
    "        for j,k in enumerate(currKeys):\n",
    "            f[k] = features[j]\n",
    "\n",
    "        c += batchSize\n",
    "        progressBar((c*100)/total,start)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def get_pd_from_embeddings_and_num (path):\n",
    "    feature_vector_test = {}\n",
    "\n",
    "    f = h5py.File(path, \"r\")\n",
    "    key1 = list(f.keys())\n",
    "    ii=0\n",
    "    for k in list(f.values()):\n",
    "        feature_vector_test[key1[ii]] = k\n",
    "        ii+=1    \n",
    "\n",
    "    new_data = {}\n",
    "    for i in range(len(feature_vector_test['0'])):\n",
    "        new_data[str(i)] = []\n",
    "    for key in feature_vector_test.keys():\n",
    "        for i in range(len(feature_vector_test[key])):\n",
    "            new_data[str(i)].append(feature_vector_test[key][i])\n",
    "\n",
    "    bert_emb_test_pd = pd.DataFrame(data = new_data)\n",
    "    f.close()\n",
    "    return bert_emb_test_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecture_vector_readme = get_readme_encoding(df[\"summary_t5\"],\"mbert_readme_data.data\",\"mbert_readme_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
